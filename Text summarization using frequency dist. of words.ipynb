{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\askhura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A series of winter blasts have struck the US state of Texas in recent days, leaving at least ten people dead in the aftermath.',\n",
       " 'On Wednesday, the second wave of the storms impacted utilities and water systems, with cities asking citizens to conserve water usage.',\n",
       " 'Reports early Wednesday indicated more than two and a half million homes and businesses were without power.',\n",
       " 'Texas is the only one of the 48 contiguous US states with its own separate power grid.One media source indicated carbon monoxide poisoning killed a mother and her child in the city of Houston.',\n",
       " 'A house fire killed a woman and three children in the city of Sugar Land.',\n",
       " 'The storm has impacted shipments of COVID-19 vaccine, according to at least one report.',\n",
       " 'Citizens were asked to conserve usage of electricity where possible.',\n",
       " 'Reports from the city of Tyler indicate roads were nearly impassible on Monday and Tuesday due to ice and snow.Earlier in the month, winter conditions contributed to a massive vehicle pile up in the city of Fort Worth, taking the lives of at least five people.',\n",
       " 'Some victims were trapped in their vehicles for hours, awaiting rescue.',\n",
       " 'During a press briefing, an emergency services official said this was a once-in-a-lifetime event.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenizing sentences from paragraph\n",
    "text = \"A series of winter blasts have struck the US state of Texas in recent days, leaving at least ten people dead in the aftermath. On Wednesday, the second wave of the storms impacted utilities and water systems, with cities asking citizens to conserve water usage. Reports early Wednesday indicated more than two and a half million homes and businesses were without power. Texas is the only one of the 48 contiguous US states with its own separate power grid.One media source indicated carbon monoxide poisoning killed a mother and her child in the city of Houston. A house fire killed a woman and three children in the city of Sugar Land. The storm has impacted shipments of COVID-19 vaccine, according to at least one report. Citizens were asked to conserve usage of electricity where possible. Reports from the city of Tyler indicate roads were nearly impassible on Monday and Tuesday due to ice and snow.Earlier in the month, winter conditions contributed to a massive vehicle pile up in the city of Fort Worth, taking the lives of at least five people. Some victims were trapped in their vehicles for hours, awaiting rescue. During a press briefing, an emergency services official said this was a once-in-a-lifetime event.\"\n",
    "sent_text = sent_tokenize(text)\n",
    "sent_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'series',\n",
       " 'of',\n",
       " 'winter',\n",
       " 'blasts',\n",
       " 'have',\n",
       " 'struck',\n",
       " 'the',\n",
       " 'us',\n",
       " 'state',\n",
       " 'of',\n",
       " 'texas',\n",
       " 'in',\n",
       " 'recent',\n",
       " 'days',\n",
       " ',',\n",
       " 'leaving',\n",
       " 'at',\n",
       " 'least',\n",
       " 'ten',\n",
       " 'people',\n",
       " 'dead',\n",
       " 'in',\n",
       " 'the',\n",
       " 'aftermath',\n",
       " '.',\n",
       " 'on',\n",
       " 'wednesday',\n",
       " ',',\n",
       " 'the',\n",
       " 'second',\n",
       " 'wave',\n",
       " 'of',\n",
       " 'the',\n",
       " 'storms',\n",
       " 'impacted',\n",
       " 'utilities',\n",
       " 'and',\n",
       " 'water',\n",
       " 'systems',\n",
       " ',',\n",
       " 'with',\n",
       " 'cities',\n",
       " 'asking',\n",
       " 'citizens',\n",
       " 'to',\n",
       " 'conserve',\n",
       " 'water',\n",
       " 'usage',\n",
       " '.',\n",
       " 'reports',\n",
       " 'early',\n",
       " 'wednesday',\n",
       " 'indicated',\n",
       " 'more',\n",
       " 'than',\n",
       " 'two',\n",
       " 'and',\n",
       " 'a',\n",
       " 'half',\n",
       " 'million',\n",
       " 'homes',\n",
       " 'and',\n",
       " 'businesses',\n",
       " 'were',\n",
       " 'without',\n",
       " 'power',\n",
       " '.',\n",
       " 'texas',\n",
       " 'is',\n",
       " 'the',\n",
       " 'only',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " '48',\n",
       " 'contiguous',\n",
       " 'us',\n",
       " 'states',\n",
       " 'with',\n",
       " 'its',\n",
       " 'own',\n",
       " 'separate',\n",
       " 'power',\n",
       " 'grid.one',\n",
       " 'media',\n",
       " 'source',\n",
       " 'indicated',\n",
       " 'carbon',\n",
       " 'monoxide',\n",
       " 'poisoning',\n",
       " 'killed',\n",
       " 'a',\n",
       " 'mother',\n",
       " 'and',\n",
       " 'her',\n",
       " 'child',\n",
       " 'in',\n",
       " 'the',\n",
       " 'city',\n",
       " 'of',\n",
       " 'houston',\n",
       " '.',\n",
       " 'a',\n",
       " 'house',\n",
       " 'fire',\n",
       " 'killed',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'and',\n",
       " 'three',\n",
       " 'children',\n",
       " 'in',\n",
       " 'the',\n",
       " 'city',\n",
       " 'of',\n",
       " 'sugar',\n",
       " 'land',\n",
       " '.',\n",
       " 'the',\n",
       " 'storm',\n",
       " 'has',\n",
       " 'impacted',\n",
       " 'shipments',\n",
       " 'of',\n",
       " 'covid-19',\n",
       " 'vaccine',\n",
       " ',',\n",
       " 'according',\n",
       " 'to',\n",
       " 'at',\n",
       " 'least',\n",
       " 'one',\n",
       " 'report',\n",
       " '.',\n",
       " 'citizens',\n",
       " 'were',\n",
       " 'asked',\n",
       " 'to',\n",
       " 'conserve',\n",
       " 'usage',\n",
       " 'of',\n",
       " 'electricity',\n",
       " 'where',\n",
       " 'possible',\n",
       " '.',\n",
       " 'reports',\n",
       " 'from',\n",
       " 'the',\n",
       " 'city',\n",
       " 'of',\n",
       " 'tyler',\n",
       " 'indicate',\n",
       " 'roads',\n",
       " 'were',\n",
       " 'nearly',\n",
       " 'impassible',\n",
       " 'on',\n",
       " 'monday',\n",
       " 'and',\n",
       " 'tuesday',\n",
       " 'due',\n",
       " 'to',\n",
       " 'ice',\n",
       " 'and',\n",
       " 'snow.earlier',\n",
       " 'in',\n",
       " 'the',\n",
       " 'month',\n",
       " ',',\n",
       " 'winter',\n",
       " 'conditions',\n",
       " 'contributed',\n",
       " 'to',\n",
       " 'a',\n",
       " 'massive',\n",
       " 'vehicle',\n",
       " 'pile',\n",
       " 'up',\n",
       " 'in',\n",
       " 'the',\n",
       " 'city',\n",
       " 'of',\n",
       " 'fort',\n",
       " 'worth',\n",
       " ',',\n",
       " 'taking',\n",
       " 'the',\n",
       " 'lives',\n",
       " 'of',\n",
       " 'at',\n",
       " 'least',\n",
       " 'five',\n",
       " 'people',\n",
       " '.',\n",
       " 'some',\n",
       " 'victims',\n",
       " 'were',\n",
       " 'trapped',\n",
       " 'in',\n",
       " 'their',\n",
       " 'vehicles',\n",
       " 'for',\n",
       " 'hours',\n",
       " ',',\n",
       " 'awaiting',\n",
       " 'rescue',\n",
       " '.',\n",
       " 'during',\n",
       " 'a',\n",
       " 'press',\n",
       " 'briefing',\n",
       " ',',\n",
       " 'an',\n",
       " 'emergency',\n",
       " 'services',\n",
       " 'official',\n",
       " 'said',\n",
       " 'this',\n",
       " 'was',\n",
       " 'a',\n",
       " 'once-in-a-lifetime',\n",
       " 'event',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##toeknizing word from paragraph\n",
    "words_in_sentence = []\n",
    "words_in_sentence = word_tokenize(text.lower())\n",
    "words_in_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\askhura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##removing stop words\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##stopwords.words(\"english\")\n",
    "## making a combined list of stop words from using stopwords from NLTK and punctuation word from Python String module\n",
    "##we can also add our own stopwords to this list if we want some special words to be trated as stop words\n",
    "\n",
    "_stop_words = set(stopwords.words(\"english\")+list(punctuation))\n",
    "_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now filtering out stop words from words_in_Sentence list\n",
    "words = [word for word in words_in_sentence if word not in _stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['series',\n",
       " 'winter',\n",
       " 'blasts',\n",
       " 'struck',\n",
       " 'us',\n",
       " 'state',\n",
       " 'texas',\n",
       " 'recent',\n",
       " 'days',\n",
       " 'leaving',\n",
       " 'least',\n",
       " 'ten',\n",
       " 'people',\n",
       " 'dead',\n",
       " 'aftermath',\n",
       " 'wednesday',\n",
       " 'second',\n",
       " 'wave',\n",
       " 'storms',\n",
       " 'impacted',\n",
       " 'utilities',\n",
       " 'water',\n",
       " 'systems',\n",
       " 'cities',\n",
       " 'asking',\n",
       " 'citizens',\n",
       " 'conserve',\n",
       " 'water',\n",
       " 'usage',\n",
       " 'reports',\n",
       " 'early',\n",
       " 'wednesday',\n",
       " 'indicated',\n",
       " 'two',\n",
       " 'half',\n",
       " 'million',\n",
       " 'homes',\n",
       " 'businesses',\n",
       " 'without',\n",
       " 'power',\n",
       " 'texas',\n",
       " 'one',\n",
       " '48',\n",
       " 'contiguous',\n",
       " 'us',\n",
       " 'states',\n",
       " 'separate',\n",
       " 'power',\n",
       " 'grid.one',\n",
       " 'media',\n",
       " 'source',\n",
       " 'indicated',\n",
       " 'carbon',\n",
       " 'monoxide',\n",
       " 'poisoning',\n",
       " 'killed',\n",
       " 'mother',\n",
       " 'child',\n",
       " 'city',\n",
       " 'houston',\n",
       " 'house',\n",
       " 'fire',\n",
       " 'killed',\n",
       " 'woman',\n",
       " 'three',\n",
       " 'children',\n",
       " 'city',\n",
       " 'sugar',\n",
       " 'land',\n",
       " 'storm',\n",
       " 'impacted',\n",
       " 'shipments',\n",
       " 'covid-19',\n",
       " 'vaccine',\n",
       " 'according',\n",
       " 'least',\n",
       " 'one',\n",
       " 'report',\n",
       " 'citizens',\n",
       " 'asked',\n",
       " 'conserve',\n",
       " 'usage',\n",
       " 'electricity',\n",
       " 'possible',\n",
       " 'reports',\n",
       " 'city',\n",
       " 'tyler',\n",
       " 'indicate',\n",
       " 'roads',\n",
       " 'nearly',\n",
       " 'impassible',\n",
       " 'monday',\n",
       " 'tuesday',\n",
       " 'due',\n",
       " 'ice',\n",
       " 'snow.earlier',\n",
       " 'month',\n",
       " 'winter',\n",
       " 'conditions',\n",
       " 'contributed',\n",
       " 'massive',\n",
       " 'vehicle',\n",
       " 'pile',\n",
       " 'city',\n",
       " 'fort',\n",
       " 'worth',\n",
       " 'taking',\n",
       " 'lives',\n",
       " 'least',\n",
       " 'five',\n",
       " 'people',\n",
       " 'victims',\n",
       " 'trapped',\n",
       " 'vehicles',\n",
       " 'hours',\n",
       " 'awaiting',\n",
       " 'rescue',\n",
       " 'press',\n",
       " 'briefing',\n",
       " 'emergency',\n",
       " 'services',\n",
       " 'official',\n",
       " 'said',\n",
       " 'once-in-a-lifetime',\n",
       " 'event']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extracting summary from words, we will use Frequency distribution of words. Creating a frequency dict of words will help. We will use freqdist module from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'city': 4, 'least': 3, 'winter': 2, 'us': 2, 'texas': 2, 'people': 2, 'wednesday': 2, 'impacted': 2, 'water': 2, 'citizens': 2, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "word_freq = FreqDist(words)\n",
    "word_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to rank words with high frequency to low freq, we will use <b>nlargest </b>from <b>heapq</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city', 'least', 'winter', 'us', 'texas']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from heapq import nlargest\n",
    "\n",
    "nlargest(5,word_freq, word_freq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 17,\n",
       "             1: 19,\n",
       "             2: 11,\n",
       "             3: 23,\n",
       "             4: 11,\n",
       "             5: 12,\n",
       "             6: 7,\n",
       "             7: 30,\n",
       "             8: 6,\n",
       "             9: 8})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "sent_index_freq_dict = defaultdict(int)\n",
    "\n",
    "for i,sent in enumerate(sent_text):               ##this enumerate will convert sent_text into index,sentence type list\n",
    "    for word in word_tokenize(sent):\n",
    "        if word in word_freq:\n",
    "            ##means word in sentence is a frequent work, its score will be added \n",
    "            sent_index_freq_dict[i] +=word_freq[word]\n",
    "sent_index_freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the dict with sentence index and their score of words , we can sort it using nlargest and get the top rated sentence to create a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reports from the city of Tyler indicate roads were nearly impassible on Monday and Tuesday due to ice and snow.Earlier in the month, winter conditions contributed to a massive vehicle pile up in the city of Fort Worth, taking the lives of at least five people.Texas is the only one of the 48 contiguous US states with its own separate power grid.One media source indicated carbon monoxide poisoning killed a mother and her child in the city of Houston.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_index_summary = nlargest(2, sent_index_freq_dict, sent_index_freq_dict.get)\n",
    "summary = ''\n",
    "for index in sents_index_summary:\n",
    "    summary += sent_text[index]\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
